# -*- coding: utf-8 -*-
"""CNN-based-classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bOwm2fKsN5xcsW03B532FJ7dYWLeMJL1
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix
import copy

def trainCNN():
    # ðŸ”¹ Define transformations
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    # ðŸ”¹ Load CIFAR-10 dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)

    classes = trainset.classes

    # ðŸ”¹ Define CNN model
    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(3, 6, kernel_size=5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
            self.fc1 = nn.Linear(16 * 5 * 5, 120)
            self.fc2 = nn.Linear(120, 84)
            self.fc3 = nn.Linear(84, 10)
            self.outputs = {}

        def forward(self, x):
            x = self.conv1(x)
            self.outputs['conv1'] = x
            x = torch.relu(x)
            x = self.pool(x)
            self.outputs['pool1'] = x
            x = self.conv2(x)
            self.outputs['conv2'] = x
            x = torch.relu(x)
            x = self.pool(x)
            self.outputs['pool2'] = x
            x = torch.flatten(x, 1)
            self.outputs['flatten'] = x
            x = self.fc1(x)
            self.outputs['fc1'] = x
            x = torch.relu(x)
            x = self.fc2(x)
            self.outputs['fc2'] = x
            x = torch.relu(x)
            x = self.fc3(x)
            self.outputs['fc3'] = x
            return x

    model = CNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ðŸ”¹ Training loop
    num_epochs = 5
    epoch_losses = []
    epoch_snapshots = {}

    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in trainloader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            epoch_losses.append(loss.item())
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        if epoch == 0 or epoch == num_epochs-1:  # Adjust as needed
          epoch_snapshots[f"epoch_{epoch+1}"] = {
              "conv1_weights": model.conv1.weight.detach().cpu().numpy(),
              "conv2_weights": model.conv2.weight.detach().cpu().numpy(),
              "fc1_weights": model.fc1.weight.detach().cpu().numpy(),
              "fc2_weights": model.fc2.weight.detach().cpu().numpy(),
              "fc3_weights": model.fc3.weight.detach().cpu().numpy()
          }
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')
    # plt.plot(epoch_losses)
    # plt.xlabel('Iterations')
    # plt.ylabel('Loss')
    # plt.title('Training Loss')
    # plt.show()

    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    print(classification_report(y_true, y_pred))
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Test Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

    # ðŸ”¹ Visualization functions


    def plot_weights_gradients(model):
        layers = [model.conv1, model.conv2, model.fc1, model.fc2, model.fc3]
        fig, axes = plt.subplots(len(layers), 2, figsize=(10, 15))
        for i, layer in enumerate(layers):
            weights = layer.weight.detach().cpu().numpy().flatten()
            gradients = layer.weight.grad.detach().cpu().numpy().flatten()
            axes[i, 0].hist(weights, bins=50, alpha=0.75, color='blue')
            axes[i, 0].set_title(f'Layer {i+1} Weights')
            axes[i, 1].hist(gradients, bins=50, alpha=0.75, color='red')
            axes[i, 1].set_title(f'Layer {i+1} Gradients')
        plt.tight_layout()
        plt.show()

    def visualize_filters(weights, title):
        # Check if these are convolutional filters (4D) or fully connected weights (2D)
        if len(weights.shape) == 4:  # Conv filters: [out_channels, in_channels, height, width]
            fig, axes = plt.subplots(1, min(6, weights.shape[0]), figsize=(15, 5))
            if weights.shape[0] == 1:  # Handle the case of a single filter
                axes = [axes]
            for i, ax in enumerate(axes):
                if i < weights.shape[0]:  # Make sure we don't go out of bounds
                    filter_img = weights[i, 0].detach().cpu().numpy()  # Visualize first channel
                    ax.imshow(filter_img, cmap='gray')
                    ax.axis('off')
            plt.suptitle(title)
            plt.show()
        else:
            # For FC layers, just show a histogram of weights
            plt.figure(figsize=(10, 5))
            plt.hist(weights.detach().cpu().numpy().flatten(), bins=50, alpha=0.75)
            plt.title(f"{title} - Weight Distribution")
            plt.xlabel("Weight Value")
            plt.ylabel("Frequency")
            plt.show()

    def visualize_feature_maps(layer_output, title):
        # Only visualize feature maps that are actually 2D (or 3D with channels)
        if len(layer_output.shape) < 4:  # Not a convolutional feature map
            print(f"Cannot visualize {title} with shape {layer_output.shape}")
            return

        fig, axes = plt.subplots(1, min(6, layer_output.shape[1]), figsize=(15, 5))
        if layer_output.shape[1] == 1:  # Handle the case of a single feature map
            axes = [axes]
        for i, ax in enumerate(axes):
            if i < layer_output.shape[1]:  # Make sure we don't go out of bounds
                ax.imshow(layer_output[0, i].detach().cpu().numpy(), cmap='gray')
                ax.axis('off')
        plt.suptitle(title)
        plt.show()

    def plot_weight_evolution(epoch_snapshots, layer_name):
        weights_ep1 = epoch_snapshots["epoch_1"][layer_name]
        weights_ep_last = epoch_snapshots[f"epoch_{num_epochs}"][layer_name]

        # Handle different dimensions for conv and FC layers
        if len(weights_ep1.shape) == 4:  # Conv layer
            # For convolutional layers, average across input channels
            w1 = weights_ep1.mean(axis=1)
            w2 = weights_ep_last.mean(axis=1)
        elif len(weights_ep1.shape) == 2:  # FC layer
            # For FC layers, reshape to make it more 2D-like for visualization
            # Take first 100 neurons if more exist
            max_neurons = min(100, weights_ep1.shape[0])
            w1 = weights_ep1[:max_neurons].reshape(-1, 10)
            w2 = weights_ep_last[:max_neurons].reshape(-1, 10)

        fig, axes = plt.subplots(1, 2, figsize=(10, 5))
        im1 = axes[0].imshow(w1, cmap="coolwarm", aspect='auto')
        axes[0].set_title(f"{layer_name} Weights - Epoch 1")
        plt.colorbar(im1, ax=axes[0])

        im2 = axes[1].imshow(w2, cmap="coolwarm", aspect='auto')
        axes[1].set_title(f"{layer_name} Weights - Epoch {num_epochs}")
        plt.colorbar(im2, ax=axes[1])

    plt.show()



    # ðŸ”¹ Test Accuracy
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy on test set: {100 * correct / total:.2f}%')

    # ðŸ”¹ Visualize weights and gradients
    plot_weight_evolution(epoch_snapshots, "conv1_weights")
    plot_weight_evolution(epoch_snapshots, "fc1_weights")

    # ðŸ”¹ Visualize feature maps
    visualize_feature_maps(model.outputs['conv1'], 'Feature Maps after Conv1')
    visualize_feature_maps(model.outputs['pool1'], 'Feature Maps after Pool1')
    visualize_feature_maps(model.outputs['conv2'], 'Feature Maps after Conv2')
    visualize_feature_maps(model.outputs['pool2'], 'Feature Maps after Pool2')
    # visualize_feature_maps(model.outputs['flatten'], 'Feature Maps after Flatten')
    # visualize_feature_maps(model.outputs['fc1'], 'Feature Maps after FC1')
    # visualize_feature_maps(model.outputs['fc2'], 'Feature Maps after FC2')
    # visualize_feature_maps(model.outputs['fc3'], 'Feature Maps after FC3')

    # ðŸ”¹ Visualize the learned filters of the first conv layer
    visualize_filters(model.conv1.weight, 'Conv1 Filters')

    # ðŸ”¹ Print layer output shapes
    print("Layer Output Shapes:")
    for key, value in model.outputs.items():
        print(f"{key}: {value.shape}")
    plot_weights_gradients(model)

def trainCNN():
    # ðŸ”¹ Define transformations
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    # ðŸ”¹ Load CIFAR-10 dataset
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)

    classes = trainset.classes

    # ðŸ”¹ Define CNN model
    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(3, 6, kernel_size=5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
            self.fc1 = nn.Linear(16 * 5 * 5, 120)
            self.fc2 = nn.Linear(120, 84)
            self.fc3 = nn.Linear(84, 10)
            self.outputs = {}

        def forward(self, x):
            x = self.conv1(x)
            self.outputs['conv1'] = x
            x = torch.relu(x)
            x = self.pool(x)
            self.outputs['pool1'] = x
            x = self.conv2(x)
            self.outputs['conv2'] = x
            x = torch.relu(x)
            x = self.pool(x)
            self.outputs['pool2'] = x
            x = torch.flatten(x, 1)
            self.outputs['flatten'] = x
            x = self.fc1(x)
            self.outputs['fc1'] = x
            x = torch.relu(x)
            x = self.fc2(x)
            self.outputs['fc2'] = x
            x = torch.relu(x)
            x = self.fc3(x)
            self.outputs['fc3'] = x
            return x

    model = CNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Add this check before training starts
    print("Initial weight check:")
    print(f"Conv1 weight sample: {model.conv1.weight[0,0,0,0].item():.6f}")
    print(f"FC1 weight sample: {model.fc1.weight[0,0].item():.6f}")

    # ðŸ”¹ Training loop
    num_epochs = 2
    epoch_losses = []
    epoch_snapshots1 = {}
    epoch_snapshots2 = {}

    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in trainloader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            epoch_losses.append(loss.item())
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # Print a weight check at the end of each epoch
        # print(f"Epoch {epoch+1} weight check:")
        # print(f"Conv1 weight sample: {model.conv1.weight[0,0,0,0].item():.6f}")
        # print(f"FC1 weight sample: {model.fc1.weight[0,0].item():.6f}")


        if epoch == 0 :  # Adjust as needed
          print("HEHEHEHEHEHEHEEH2")
          epoch_snapshots1[f"epoch_{epoch+1}"] = {
              "conv1_weights": copy.deepcopy(model.conv1.weight.detach().cpu().numpy()),
              "conv2_weights": copy.deepcopy(model.conv2.weight.detach().cpu().numpy()),
              "fc1_weights": copy.deepcopy(model.fc1.weight.detach().cpu().numpy()),
              "fc2_weights": copy.deepcopy(model.fc2.weight.detach().cpu().numpy()),
              "fc3_weights": copy.deepcopy(model.fc3.weight.detach().cpu().numpy())
          }
        print(epoch_snapshots1)

        if epoch == num_epochs-1:  # Adjust as needed
          print("HEHEHEHEHEHEHEEH")
          epoch_snapshots2[f"epoch_{epoch+1}"] = {
              "conv1_weights": copy.deepcopy(model.conv1.weight.detach().cpu().numpy()),
              "conv2_weights": copy.deepcopy(model.conv2.weight.detach().cpu().numpy()),
              "fc1_weights": copy.deepcopy(model.fc1.weight.detach().cpu().numpy()),
              "fc2_weights": copy.deepcopy(model.fc2.weight.detach().cpu().numpy()),
              "fc3_weights": copy.deepcopy(model.fc3.weight.detach().cpu().numpy())
          }
          print(model.conv1.weight.detach().cpu().numpy())
        print("MIC CHECK")
        print(epoch_snapshots1)
        print(epoch_snapshots2)

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader)}')
    # plt.plot(epoch_losses)
    # plt.xlabel('Iterations')
    # plt.ylabel('Loss')
    # plt.title('Training Loss')
    # plt.show()

    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    print(classification_report(y_true, y_pred))
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Test Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

    # ðŸ”¹ Visualization functions


    def plot_weights_gradients(model):
        layers = [model.conv1, model.conv2, model.fc1, model.fc2, model.fc3]
        fig, axes = plt.subplots(len(layers), 2, figsize=(10, 15))
        for i, layer in enumerate(layers):
            weights = layer.weight.detach().cpu().numpy().flatten()
            gradients = layer.weight.grad.detach().cpu().numpy().flatten()
            axes[i, 0].hist(weights, bins=50, alpha=0.75, color='blue')
            axes[i, 0].set_title(f'Layer {i+1} Weights')
            axes[i, 1].hist(gradients, bins=50, alpha=0.75, color='red')
            axes[i, 1].set_title(f'Layer {i+1} Gradients')
        plt.tight_layout()
        plt.show()

    def visualize_filters(weights, title):
      # Check if these are convolutional filters (4D) or fully connected weights (2D)
      if len(weights.shape) == 4:  # Conv filters: [out_channels, in_channels, height, width]
          fig, axes = plt.subplots(1, min(6, weights.shape[0]), figsize=(15, 5))
          if weights.shape[0] == 1:  # Handle the case of a single filter
              axes = [axes]
          for i, ax in enumerate(axes):
              if i < weights.shape[0]:  # Make sure we don't go out of bounds
                  # For RGB input (3 channels), create a RGB visualization
                  if weights.shape[1] == 3:
                      # Normalize each channel separately for better visualization
                      r = weights[i, 0].detach().cpu().numpy()
                      g = weights[i, 1].detach().cpu().numpy()
                      b = weights[i, 2].detach().cpu().numpy()

                      # Normalize to [0, 1] for each channel
                      r_norm = (r - r.min()) / (r.max() - r.min() + 1e-8)
                      g_norm = (g - g.min()) / (g.max() - g.min() + 1e-8)
                      b_norm = (b - b.min()) / (b.max() - b.min() + 1e-8)

                      # Stack the normalized channels into an RGB image
                      rgb_filter = np.stack([r_norm, g_norm, b_norm], axis=2)
                      ax.imshow(rgb_filter)
                  else:
                      # For grayscale filters, just show the first input channel
                      filter_img = weights[i, 0].detach().cpu().numpy()
                      ax.imshow(filter_img, cmap='gray')
                  ax.axis('off')
          plt.suptitle(title)
          plt.show()
      else:
          # For FC layers, just show a histogram of weights
          plt.figure(figsize=(10, 5))
          plt.hist(weights.detach().cpu().numpy().flatten(), bins=50, alpha=0.75)
          plt.title(f"{title} - Weight Distribution")
          plt.xlabel("Weight Value")
          plt.ylabel("Frequency")
          plt.show()

    def visualize_feature_maps(layer_output, title):
      # Only visualize feature maps that are actually 2D (or 3D with channels)
      if len(layer_output.shape) < 4:  # Not a convolutional feature map
          print(f"Cannot visualize {title} with shape {layer_output.shape}")
          return

      # Get the number of feature maps to display (min of 6 or number available)
      num_maps = min(6, layer_output.shape[1])

      # Create a subplot for each feature map
      fig, axes = plt.subplots(1, num_maps, figsize=(15, 5))

      # Handle the case of a single feature map
      if num_maps == 1:
          axes = [axes]

      # Display each feature map
      for i in range(num_maps):
          feature_map = layer_output[0, i].detach().cpu().numpy()
          axes[i].imshow(feature_map, cmap='gray')
          axes[i].axis('off')

      plt.suptitle(title)
      plt.tight_layout()
      plt.show()


    def plot_weight_evolution(epoch_snapshots1, epoch_snapshots2, layer_name):
      weights_ep1 = epoch_snapshots1["epoch_1"][layer_name]
      weights_ep_last = epoch_snapshots2[f"epoch_{num_epochs}"][layer_name]

      # Add this near the end of your training loop
      print("Weight change check:")
      print(f"First layer weight sample before: {weights_ep1}")
      # Train for an epoch...
      print(f"First layer weight sample after: {weights_ep_last}")


      # Handle different dimensions for conv and FC layers
      if len(weights_ep1.shape) == 4:  # Conv layer
          # For convolutional layers, we need to reshape to 2D for visualization
          # Average across input channels
          w1 = weights_ep1.mean(axis=1).reshape(weights_ep1.shape[0], -1)
          w2 = weights_ep_last.mean(axis=1).reshape(weights_ep_last.shape[0], -1)
      elif len(weights_ep1.shape) == 2:  # FC layer
          # For FC layers, reshape to make it more 2D-like for visualization
          # Take first 100 neurons if more exist
          max_neurons = min(100, weights_ep1.shape[0])
          w1 = weights_ep1[:max_neurons].reshape(max_neurons, -1)
          w2 = weights_ep_last[:max_neurons].reshape(max_neurons, -1)

      fig, axes = plt.subplots(1, 2, figsize=(12, 5))
      print(w1,w2)
      # Plot the weights at epoch 1
      im1 = axes[0].imshow(w1, cmap="coolwarm", aspect='auto')
      axes[0].set_title(f"{layer_name} Weights - Epoch 1")
      fig.colorbar(im1, ax=axes[0])

      # Plot the weights at last epoch
      im2 = axes[1].imshow(w2, cmap="coolwarm", aspect='auto')
      axes[1].set_title(f"{layer_name} Weights - Epoch {num_epochs}")
      fig.colorbar(im2, ax=axes[1])

      plt.tight_layout()
      plt.show()


    # ðŸ”¹ Test Accuracy
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in testloader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy on test set: {100 * correct / total:.2f}%')

    # ðŸ”¹ Visualize weights and gradients
    # print("HUHUHHUHHUHHUH")
    # print(epoch_snapshots1)
    # print(epoch_snapshots2)
    plot_weight_evolution(epoch_snapshots1, epoch_snapshots2, "conv1_weights")
    plot_weight_evolution(epoch_snapshots1, epoch_snapshots2, "fc1_weights")

    # ðŸ”¹ Visualize feature maps
    visualize_feature_maps(model.outputs['conv1'], 'Feature Maps after Conv1')
    visualize_feature_maps(model.outputs['pool1'], 'Feature Maps after Pool1')
    visualize_feature_maps(model.outputs['conv2'], 'Feature Maps after Conv2')
    visualize_feature_maps(model.outputs['pool2'], 'Feature Maps after Pool2')


    # ðŸ”¹ Visualize the learned filters of the first conv layer
    visualize_filters(model.conv1.weight, 'Conv1 Filters')

    # ðŸ”¹ Print layer output shapes
    print("Layer Output Shapes:")
    for key, value in model.outputs.items():
        print(f"{key}: {value.shape}")
    plot_weights_gradients(model)

trainCNN()

trainCNN()

# Run the training function
trainCNN()

def trainCNN():
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)

    class CNN(nn.Module):
      def __init__(self):
          super(CNN, self).__init__()
          self.conv1 = nn.Conv2d(3, 32, kernel_size=5)  # Updated to 32 filters
          self.bn1 = nn.BatchNorm2d(32)  # Added BatchNorm
          self.pool = nn.MaxPool2d(2, 2)  # Stride 2

          self.conv2 = nn.Conv2d(32, 64, kernel_size=5)  # Increased filters from 16 â†’ 64
          self.bn2 = nn.BatchNorm2d(64)  # Added BatchNorm

          self.fc1 = nn.Linear(64 * 5 * 5, 512)  # Adjusted input size, increased neurons from 120 â†’ 512
          self.drop1 = nn.Dropout(0.5)  # Added Dropout

          self.fc2 = nn.Linear(512, 256)  # Increased neurons from 84 â†’ 256
          self.drop2 = nn.Dropout(0.5)  # Added Dropout

          self.fc3 = nn.Linear(256, 10)  # Output layer

          self.outputs = {}

      def forward(self, x):
        x = self.conv1(x)
        self.outputs['conv1'] = x
        x = self.bn1(x)
        x = torch.relu(x)
        x = self.pool(x)
        self.outputs['pool1'] = x

        x = self.conv2(x)
        self.outputs['conv2'] = x
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.pool(x)
        self.outputs['pool2'] = x

        x = torch.flatten(x, 1)
        self.outputs['flatten'] = x

        x = self.fc1(x)
        self.outputs['fc1'] = x
        x = torch.relu(x)
        x = self.drop1(x)

        x = self.fc2(x)
        self.outputs['fc2'] = x
        x = torch.relu(x)
        x = self.drop2(x)

        x = self.fc3(x)
        self.outputs['fc3'] = x

        return x


    model = CNN()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    num_epochs = 20
    epoch_snapshots1 = {}
    epoch_snapshots2 = {}

    # âœ… Print Initial Weights
    print("\nInitial weight check:")
    print(f"Conv1 weight sample: {model.conv1.weight[0,0,0,0].item():.6f}")
    print(f"FC1 weight sample: {model.fc1.weight[0,0].item():.6f}")

    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in trainloader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # âœ… Store Model Weight Snapshots
        snapshot = {
            "conv1_weights": model.conv1.weight.detach().cpu().numpy().copy(),
            "conv2_weights": model.conv2.weight.detach().cpu().numpy().copy(),
            # "conv3_weights": model.conv3.weight.detach().cpu().numpy().copy(),
            "fc1_weights": model.fc1.weight.detach().cpu().numpy().copy(),
            "fc2_weights": model.fc2.weight.detach().cpu().numpy().copy(),
            "fc3_weights": model.fc3.weight.detach().cpu().numpy().copy(),
        }

        if epoch == 0:
            epoch_snapshots1 = snapshot
        if epoch == num_epochs - 1:
            epoch_snapshots2 = snapshot

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.6f}')

    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for images, labels in testloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    print(classification_report(y_true, y_pred))
    accuracy = accuracy_score(y_true, y_pred)
    print(f"Test Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

    # âœ… Plot Weight Matrices
    def plot_weight_matrix(weights, title):
        if len(weights.shape) == 4:  # Conv layers: (out_channels, in_channels, kernel_size, kernel_size)
            weights = weights[:, 0, :, :].reshape(weights.shape[0], -1)  # Flatten kernels
        elif len(weights.shape) == 2:  # FC layers: (neurons, inputs)
            max_neurons = min(100, weights.shape[0])  # Limit number of neurons for visualization
            weights = weights[:max_neurons, :]

        plt.figure(figsize=(10, 6))
        plt.imshow(weights, cmap="coolwarm", aspect='auto')
        plt.colorbar()
        plt.title(title)
        plt.xlabel("Inputs / Kernel Weights")
        plt.ylabel("Neurons / Filters")
        plt.show()

        # âœ… Plot Evolution of Weights
    # import matplotlib.pyplot as plt

    for layer_name in ["conv1_weights", "conv2_weights", "fc1_weights", "fc2_weights", "fc3_weights"]:
        print(f"\nðŸ”¹ Plotting {layer_name} at Epoch 1 and {num_epochs} ðŸ”¹")
        plot_weight_matrix(epoch_snapshots1[layer_name], f"{layer_name} - Epoch 1")
        plot_weight_matrix(epoch_snapshots2[layer_name], f"{layer_name} - Epoch {num_epochs}")

    def visualize_feature_maps(layer_output, title):
      # Only visualize feature maps that are actually 2D (or 3D with channels)
      if len(layer_output.shape) < 4:  # Not a convolutional feature map
          print(f"Cannot visualize {title} with shape {layer_output.shape}")
          return

      # Get the number of feature maps to display (min of 6 or number available)
      num_maps = min(6, layer_output.shape[1])

      # Create a subplot for each feature map
      fig, axes = plt.subplots(1, num_maps, figsize=(15, 5))

      # Handle the case of a single feature map
      if num_maps == 1:
          axes = [axes]

      # Display each feature map
      for i in range(num_maps):
          feature_map = layer_output[0, i].detach().cpu().numpy()
          axes[i].imshow(feature_map, cmap='gray')
          axes[i].axis('off')

      plt.suptitle(title)
      plt.tight_layout()
      plt.show()

    # ðŸ”¹ Visualize feature maps
    visualize_feature_maps(model.outputs['conv1'], 'Feature Maps after Conv1')
    visualize_feature_maps(model.outputs['pool1'], 'Feature Maps after Pool1')
    visualize_feature_maps(model.outputs['conv2'], 'Feature Maps after Conv2')
    visualize_feature_maps(model.outputs['pool2'], 'Feature Maps after Pool2')
    # visualize_feature_maps(model.outputs['conv3'], 'Feature Maps after Conv3')
    # visualize_feature_maps(model.outputs['gap'], 'Feature Maps after GAP')

    print("\nTraining complete.")

trainCNN()